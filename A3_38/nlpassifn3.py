# -*- coding: utf-8 -*-
"""NLPassifn3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TGPH6nE0B2FuZwTZm4blHnwVVT4pvhVb
"""

pip install sentence-transformers

pip install transformers

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import mean_squared_error
from sentence_transformers import evaluation
from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses
from torch.utils.data import DataLoader
from sentence_transformers import evaluation
from scipy.stats import pearsonr

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from google.colab import drive
drive.mount('/content/drive')

file_path='/content/drive/MyDrive/NLPassign3/A3_task1_data_files/train.csv'

scores=[]
input_sentenses1=[]
input_sentenses2=[]
import csv
with open(file_path, 'r', newline='') as csvfile:
    csvFil = csv.reader(csvfile)
    for lines in csvFil:
        arr=lines[0].split('\t')
        if(len(arr)==3):

          try:
            scores.append(float(arr[0]))
          except:
            print(arr[0])
          input_sentenses1.append(arr[1])
          input_sentenses2.append(arr[2])

scores=scores
input_sentenses1=input_sentenses1[1:]
input_sentenses2=input_sentenses2[1:]

len(input_sentenses1)

inputsentense=[]
for i in range(len(input_sentenses1)):
  inputsentense.append(input_sentenses1[i]+' [SEP] ' +input_sentenses2[i])

model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name)

def get_bert_embeddings(input_text):
    tokens = tokenizer.encode_plus(input_text, add_special_tokens=True, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**tokens)
        # print(outputs)
    # embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze(0)
    embeddings = outputs[0]
    return embeddings.numpy()

import numpy as np
embeddings_file = '/content/drive/MyDrive/NLPassign3/A3_task1_data_files/embeddings.npz'

# sentence1_embedding=[get_bert_embeddings(sentence) for sentence in input_sentenses1]
# sentence2_embedding=[get_bert_embeddings(sentence) for sentence in input_sentenses2]
# np.savez(embeddings_file, embeddings_1=sentence1_embedding, embeddings_2=sentence2_embedding)

embeddings_npz = np.load(embeddings_file)
sentence1_embedding = embeddings_npz['embeddings_1']
sentence2_embedding = embeddings_npz['embeddings_2']
print("Embeddings loaded from file.")

validscores=[]
valid_sentenses1=[]
valid_sentenses2=[]
devpath="/content/drive/MyDrive/NLPassign3/A3_task1_data_files/dev.csv"
with open(devpath, 'r', newline='') as csvfile:
    csvFil = csv.reader(csvfile)
    for lines in csvFil:
        arr=lines[0].split('\t')
        if(len(arr)==3):

          try:
            validscores.append(float(arr[0]))
          except:
            print(arr[0])
          valid_sentenses1.append(arr[1])
          valid_sentenses2.append(arr[2])


valid_sentenses1=valid_sentenses1[1:]
valid_sentenses2=valid_sentenses2[1:]

X = [torch.cat([torch.tensor(sentence1_embedding[i]).reshape(-1), torch.tensor(sentence2_embedding[i]).reshape(-1)]) for i in range(len(sentence1_embedding))]

valid1_embedding=[get_bert_embeddings(sentence) for sentence in valid_sentenses1]
valid2_embedding=[get_bert_embeddings(sentence) for sentence in valid_sentenses2]
X_valid = [torch.cat([torch.tensor(valid1_embedding[i]).reshape(-1), torch.tensor(valid2_embedding[i]).reshape(-1)]) for i in range(len(valid1_embedding))]

class LinearModel(nn.Module):
    def __init__(self, input_size):
        super(LinearModel, self).__init__()
        self.linear = nn.Linear(input_size, 1)

    def forward(self, x):
        return self.linear(x)

Linearmodel = LinearModel(X[0].shape[0])

loss_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(Linearmodel.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                                 mode='min', factor=0.01,
                                                 patience=1)

print(X[0].shape[0])

from tqdm import tqdm_notebook
import time
num_epochs = 1000

traning_loss=[]
validation_loss=[]
epoch_bar=tqdm_notebook(desc='training routine',
                          total=num_epochs,
                        position=0)
for epoch in range(num_epochs):
    Linearmodel.train()
    optimizer.zero_grad()
    outputs = Linearmodel(torch.stack(X))
    loss = loss_function(outputs.squeeze(), torch.tensor(scores, dtype=torch.float))
    loss.backward()
    optimizer.step()
    epoch_bar.update()
    traning_loss.append(loss.item())


    # Linearmodel.eval()
    with torch.no_grad():
        outputs = Linearmodel(torch.stack(X_valid))
        # predictions = outputs.squeeze().numpy()
        loss_v = loss_function(outputs.squeeze(), torch.tensor(validscores, dtype=torch.float))
        validation_loss.append(loss_v.item())

Linearmodel.eval()
with torch.no_grad():
    outputs = Linearmodel(torch.stack(X_valid))
    predictions = outputs.squeeze().numpy()

mse = mean_squared_error(validscores, predictions)
print("Mean Squared Error:", mse)

model_path_linear = '/content/drive/MyDrive/NLPassign3/A3_task1_data_files/linearmodel'

# Save the trained model
torch.save(Linearmodel.state_dict(), model_path_linear)

import matplotlib.pyplot as plt

def plot_array(arr,title):
    """
    Plots an array of numbers.

    Parameters:
        arr (list or numpy.ndarray): Array of numbers to be plotted.
    """
    plt.plot(arr)
    plt.xlabel('epoches')
    plt.ylabel('loss')
    plt.title(f'Loss plot ({title})')
    # plt.grid(True)
    plt.show()

traning_loss[1]

plot_array(traning_loss,'traning')

plot_array(validation_loss,'validation')

Linearmodel.eval()
with torch.no_grad():
    outputs = Linearmodel(torch.stack(X_valid))
    predictions = outputs.squeeze().numpy()

mse = mean_squared_error(validscores, predictions)
print("Mean Squared Error:", mse)
pearson_corr = pearsonr(validscores, predictions)
print("Pearson correlation coefficient:", pearson_corr)







model = BertForSequenceClassification.from_pretrained(model_name,num_labels=1)
tokens = tokenizer.encode_plus(inputsentense[0], add_special_tokens=True, return_tensors='pt')
with torch.no_grad():
    outputs = model(**tokens,)
    # print(outputs)
# embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze(0)
embeddings = outputs[0]
print(embeddings)







# np.savez("/content/drive/MyDrive/NLPassign3/A3_task1_data_files/embeddings1.npz", embeddings_1=inputseq)
# inputseq=[[get_bert_embeddings(sentence) for sentence in inputsentense]]
inputseq=[]
# train_inputids=[]

for i in inputsentense:
  tokens = tokenizer.encode_plus(i, add_special_tokens=True, return_tensors='pt')
  inputseq.append(tokens)

# inputseq[0]

validsenten=[]
for i in range(len(valid_sentenses1)):
  validsenten.append(valid_sentenses1[i]+' [SEP] '+valid_sentenses2[i])

# vads=[[get_bert_embeddings(sentence) for sentence in validsenten]]
vads=[]
for i in validsenten:
  tokens = tokenizer.encode_plus(i, add_special_tokens=True, return_tensors='pt')
  vads.append(tokens)

# https://discuss.huggingface.co/t/finetune-pretrained-bert-for-custom-regression-task/18562/4
from transformers import PreTrainedModel, AutoModel
class BertSimilarity(PreTrainedModel):
    def __init__(self,config):
        super().__init__(config)
        self.bert = AutoModel.from_config(config)
        self.linear = nn.Linear(config.hidden_size, config.num_labels)

    def forward(self, x):
        arr=[]
        for i in x:
          out=self.bert(**i)
          arr.append(out.last_hidden_state)
        # pooled_output = outputs[1]
        # sequence_output = outputs.last_hidden_state
        output = self.linear(arr)
        return output


    # def __init__(self,bertmodel):
    #     # super(BertForRegression, self).__init__()

    #     # Change the output layer for regression
    #     self.regressor = nn.Linear(config.hidden_size, 1)
    #     self.config = config

    # def forward(self, input_ids, attention_mask=None, token_type_ids=None):
    #     outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
    #     pooled_output = outputs[1]  # Use the pooled output
    #     output = self.regressor(pooled_output)
    #     return output
config = BertForSequenceClassification.config_class.from_pretrained('bert-base-uncased')
similarity_model=BertSimilarity(config).to(device)

loss_function = torch.nn.MSELoss()
optimizer = torch.optim.Adam(similarity_model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,
                                                 mode='min', factor=0.5,
                                                 patience=1)

inputseq

from tqdm import tqdm_notebook
import time
num_epochs = 3

traning_loss=[]
validation_loss=[]
epoch_bar=tqdm_notebook(desc='training routine',
                          total=num_epochs,
                        position=0)
for epoch in range(num_epochs):
    similarity_model.train()
    optimizer.zero_grad()
    outputs = similarity_model(inputseq)
    loss = loss_function(outputs.squeeze(), torch.tensor(scores, dtype=torch.float))
    loss.backward()
    optimizer.step()
    epoch_bar.update()
    traning_loss.append(loss.item())


    # Linearmodel.eval()
    with torch.no_grad():
        outputs = similarity_model(vads)
        # predictions = outputs.squeeze().numpy()
        loss_v = loss_function(outputs.squeeze(), torch.tensor(validscores, dtype=torch.float))
        validation_loss.append(loss_v.item())

# similarity_model.eval()
# with torch.no_grad():
#     outputs = similarity_model(torch.stack(torch.tensor(vads)))
#     predictions = outputs.squeeze().numpy()

# mse = mean_squared_error(validscores, predictions)
# print("Mean Squared Error:", mse)

similarity_model.eval()
with torch.no_grad():
    outputs = similarity_model(torch.stack(vads))
    predictions = outputs.squeeze().numpy()

mse = mean_squared_error(validscores, predictions)
print("Mean Squared Error:", mse)
pearson_corr = pearsonr(validscores, predictions)
print("Pearson correlation coefficient:", pearson_corr)

"""Cosine similarity"""

from sentence_transformers import SentenceTransformer, util
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity

model_name = 'stsb-roberta-large'
model = SentenceTransformer(model_name)

embeddings_1 = model.encode(valid_sentenses1, convert_to_tensor=True)
embeddings_2 = model.encode(valid_sentenses2, convert_to_tensor=True)

cosine_similarities = util.pytorch_cos_sim(embeddings_1, embeddings_2).cpu().numpy()

cosine_similarities.shape

len(validscores)

diagonal_similarity = np.diag(cosine_similarities)
t_sorre=[i/5 for i in validscores]
mse = mean_squared_error(t_sorre, diagonal_similarity)
print("Mean Squared Error:", mse)

pearson_corr = pearsonr(t_sorre, diagonal_similarity)
print("Pearson correlation coefficient:", pearson_corr)

"""#fine tune model

"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from torch.utils.data.dataloader import default_collate
from sentence_transformers.readers import InputExample

def custom_collate(batch):

    if isinstance(batch[0], InputExample):
        # Extract features from InputExample objects
        features = [(example.texts[0],example.texts[0],example.label) for example in batch]
        return features
    else:
        # Use default collate for other data types
        return default_collate(batch)

import torch
from torch.utils.data import Dataset, DataLoader

# class SimilarityDataset(Dataset):
#     def __init__(self, sentence1_embedding, sentence2_embedding, similarity_scores):
#         self.sentence1_embedding = sentence1_embedding
#         self.sentence2_embedding = sentence2_embedding
#         self.similarity_scores = similarity_scores

#     def __len__(self):
#         return len(self.sentence1_embedding)

#     def __getitem__(self, idx):
#         sentence_1 = self.sentence1_embedding[idx]
#         sentence_2 = self.sentence2_embedding[idx]
#         similarity_score = self.similarity_scores[idx]

#         return sentence_1, sentence_2, self.similarity_scores[idx]



# make a dataset
def makdataset(sentense1,sentense2,label):
  arr=[]
  for i in range(len(sentense1)):
    arr.append(InputExample(texts=[sentense1[i],sentense2[i]], label=(label[i])/5))
    # arr.append((sentense1[i],sentense2[i], (label[i])/5))

  return arr



dataset = makdataset(input_sentenses1,input_sentenses2,scores)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True,collate_fn=custom_collate)

validation_dataset=makdataset(valid_sentenses1,valid_sentenses2,validscores)

valid_dataloader= DataLoader(validation_dataset, batch_size=32, shuffle=True,collate_fn=custom_collate)

model_name = 'stsb-roberta-large'
model1AC = SentenceTransformer(model_name).to(device)

cosine_loss = losses.CosineSimilarityLoss(model1AC)

estscoe=[i/5 for i in validscores]
evaluator = evaluation.EmbeddingSimilarityEvaluator(valid_sentenses1,valid_sentenses2,estscoe)

num_epochs = 10
warmup_steps = int(len(dataloader) * num_epochs * 0.1)
train_loss=[]
valid_loss=[]
for i in range(num_epochs):
  embeddings_1_ac = model1AC.encode(input_sentenses1, convert_to_tensor=True)
  embeddings_2_ac = model1AC.encode(input_sentenses2, convert_to_tensor=True)
  cosine_similarities = util.pytorch_cos_sim(embeddings_1_ac, embeddings_2_ac).cpu().numpy()

  # print("Cosine Similarity:", cosine_similarity.item())
  diagonal_similarity = np.diag(cosine_similarities)
  t_sorre=[i/5 for i in scores]
  mse = mean_squared_error(t_sorre, diagonal_similarity)
  print("Mean Squared Error:", mse)
  train_loss.append(mse)
  model1AC.fit(train_objectives=[(dataloader, cosine_loss)],
          evaluator=evaluator,
          epochs=1,
          evaluation_steps=5000,
          warmup_steps=warmup_steps,
          output_path='fine_tuned_model')

  embeddings_1_ac = model1AC.encode(valid_sentenses1, convert_to_tensor=True)
  embeddings_2_ac = model1AC.encode(valid_sentenses2, convert_to_tensor=True)
  cosine_similarities = util.pytorch_cos_sim(embeddings_1_ac, embeddings_2_ac).cpu().numpy()

  # print("Cosine Similarity:", cosine_similarity.item())
  diagonal_similarity = np.diag(cosine_similarities)
  t_sorre=[i/5 for i in validscores]
  mse = mean_squared_error(t_sorre, diagonal_similarity)
  print("Mean Squared Error:", mse)
  valid_loss.append(mse)

plot_array(train_loss,'traning')

plot_array(valid_loss,'validation')

model_path="/content/drive/MyDrive/NLPassign3/model1AC_10epoch"
model1AC.save(model_path)

evaluation_result = evaluator(model1AC, output_path='fine_tuned_model')
print("Performance on the validation set:", evaluation_result)

diagonal_similarity = np.diag(cosine_similarities)
t_sorre=[i/5 for i in validscores]
mse = mean_squared_error(t_sorre, diagonal_similarity)
print("Mean Squared Error:", mse)

pearson_corr = pearsonr(t_sorre, diagonal_similarity)
print("Pearson correlation coefficient:", pearson_corr)

"""**code for test**

"""

file_path="/content/drive/MyDrive/NLPassign3/A3_task1_data_files/sample_test.csv"

ids=[]
test_sentenses1=[]
test_sentenses2=[]
zp=0
with open(file_path, 'r', newline='') as csvfile:
    csvFil = csv.reader(csvfile)
    for lines in csvFil:
        if zp==0:
          zp=1
          continue
        arr=lines[0].split('\t')
        ids.append(arr[0])
        test_sentenses1.append(arr[1])
        test_sentenses2.append(arr[2])

"""**load the model**"""

model_path="/content/drive/MyDrive/NLPassign3/model1AC_10epoch"
model1AC=SentenceTransformer(model_path)

embeddings_1_test = model1AC.encode(test_sentenses1, convert_to_tensor=True)
embeddings_2_test = model1AC.encode(test_sentenses2, convert_to_tensor=True)
cosine_similarities = util.pytorch_cos_sim(embeddings_1_test, embeddings_2_test).cpu().numpy()


diagonal_similarity = np.diag(cosine_similarities)*5
print(diagonal_similarity)

"""**save the result in a file**"""

file_name="/content/sample_demo.csv"

fidata = [ids,diagonal_similarity,test_sentenses1,test_sentenses2]

data=[]
data.append(['id','score','setence1','sentence2'])
for i in range(len(ids)):
  data.append([ids[i],diagonal_similarity[i],test_sentenses1[i],test_sentenses2[i]])


with open(file_name, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(data)


print("Data has been written to", file_name)

"""#for 1A"""

def get_bert_embeddings(input_text):
    tokens = tokenizer.encode_plus(input_text, add_special_tokens=True, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**tokens)
    # embeddings = torch.mean(outputs.last_hidden_state, dim=1).squeeze(0)
    embeddings = torch.mean(outputs[0], dim=1).squeeze(0)
    return embeddings.numpy()

embedding1A=[get_bert_embeddings(sentence) for sentence in test_sentenses1]
embedding2A=[get_bert_embeddings(sentence) for sentence in test_sentenses2]

X = [torch.cat([torch.tensor(embedding1A[i]).reshape(-1), torch.tensor(embedding2A[i]).reshape(-1)]) for i in range(len(embedding1A))]

path_model="/content/drive/MyDrive/NLPassign3/A3_task1_data_files/linearmodel"
Linearmodel = LinearModel(X[0].shape[0])
Linearmodel.load_state_dict(torch.load(path_model))

Linearmodel.eval()
with torch.no_grad():
    outputs = Linearmodel(torch.stack(X_valid))
    predictions = outputs.squeeze().numpy()

print(predictions)

file_name="/content/sample_demo_1.csv"

data=[]
data.append(['id','score','setence1','sentence2'])
for i in range(len(ids)):
  data.append([ids[i],predictions[i],test_sentenses1[i],test_sentenses2[i]])


with open(file_name, mode='w', newline='') as file:
    writer = csv.writer(file)
    writer.writerows(data)


print("Data has been written to", file_name)

