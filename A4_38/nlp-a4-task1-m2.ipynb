{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"80939ca552a1428d94d64e8495f8821e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4bbe60920874f0cb9e584f552606c7d","IPY_MODEL_ff4bf1fe28804a9aa11176d8aa1315df","IPY_MODEL_40658851460b44b29e77165906206c79"],"layout":"IPY_MODEL_af0b65bf76154b95a7ba219945abc170"}},"e4bbe60920874f0cb9e584f552606c7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8db538f6e5943e3a9926952f28f26cf","placeholder":"​","style":"IPY_MODEL_70a3207984bf4074b3d0e6e639da78cd","value":"  0%"}},"ff4bf1fe28804a9aa11176d8aa1315df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_0962cac23ce34a94b509d08a1a21c417","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_21a93957d6d04edeac30c576671d5ccd","value":0}},"40658851460b44b29e77165906206c79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09d379ce6b4e42a38984214698049288","placeholder":"​","style":"IPY_MODEL_dcd9ee673b8e4bbf8de8b54785f2e0a5","value":" 0/5 [00:00&lt;?, ?it/s]"}},"af0b65bf76154b95a7ba219945abc170":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8db538f6e5943e3a9926952f28f26cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70a3207984bf4074b3d0e6e639da78cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0962cac23ce34a94b509d08a1a21c417":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21a93957d6d04edeac30c576671d5ccd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09d379ce6b4e42a38984214698049288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dcd9ee673b8e4bbf8de8b54785f2e0a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8180274,"sourceType":"datasetVersion","datasetId":4842971}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import BertForSequenceClassification\nimport numpy as np\nfrom sklearn.metrics import f1_score, accuracy_score\nimport matplotlib.pyplot as plt\nfrom torch import Tensor\nfrom torch.nn import Transformer\nimport math\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\nfrom typing import Iterable, List\nfrom tqdm.auto import tqdm\nimport csv\nfrom nltk.translate.bleu_score import sentence_bleu","metadata":{"id":"hUlBn0dTFHDb","execution":{"iopub.status.busy":"2024-04-21T02:26:39.078130Z","iopub.execute_input":"2024-04-21T02:26:39.078772Z","iopub.status.idle":"2024-04-21T02:26:48.449704Z","shell.execute_reply.started":"2024-04-21T02:26:39.078739Z","shell.execute_reply":"2024-04-21T02:26:48.448924Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_path = \"/kaggle/input/nlp-a4-data/train_file.json\"\nval_path = \"/kaggle/input/nlp-a4-data/val_file.json\"\n\nwith open(train_path, \"r\") as file:\n    trainingData = json.load(file)\n\nwith open(val_path, \"r\") as file:\n    valData = json.load(file)\n","metadata":{"id":"tF-ESrbnF6Y0","execution":{"iopub.status.busy":"2024-04-21T02:26:48.451311Z","iopub.execute_input":"2024-04-21T02:26:48.451774Z","iopub.status.idle":"2024-04-21T02:26:48.801085Z","shell.execute_reply.started":"2024-04-21T02:26:48.451748Z","shell.execute_reply":"2024-04-21T02:26:48.800067Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(type(trainingData))\nprint(len(trainingData))\nprint(trainingData[0])\nprint(type(valData))\nprint(len(valData))\nprint(valData[0])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5QkQqry6HZ8i","outputId":"23bf8d3a-b326-4a44-fd1d-7b6664cac019","execution":{"iopub.status.busy":"2024-04-21T02:26:48.802256Z","iopub.execute_input":"2024-04-21T02:26:48.802538Z","iopub.status.idle":"2024-04-21T02:26:48.808758Z","shell.execute_reply.started":"2024-04-21T02:26:48.802513Z","shell.execute_reply":"2024-04-21T02:26:48.807789Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'list'>\n6740\n{'episode': 'utterance_3492', 'speakers': ['Phoebe', 'Eric', 'Phoebe', 'Eric', 'Phoebe'], 'emotions': ['surprise', 'fear', 'surprise', 'sadness', 'disgust'], 'utterances': ['You-you\\x85you had sex with Ursula?!', 'Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and', \"You didn't notice she was wearing different clothes?!\", 'Well I was just so excited to see you.', \"Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\"], 'triggers': [1.0, 1.0, 0.0, 0.0, 0.0]}\n<class 'list'>\n843\n{'episode': 'utterance_3421', 'speakers': ['Chandler', 'Joey', 'Chandler', 'Joey', 'Joey', 'Chandler', 'Joey', 'Joey', 'Joey', 'Chandler', 'Joey', 'Chandler', 'Joey', 'Chandler', 'Joey', 'Chandler', 'Joey'], 'emotions': ['anger', 'neutral', 'neutral', 'surprise', 'anger', 'disgust', 'neutral', 'neutral', 'neutral', 'anger', 'fear', 'surprise', 'neutral', 'sadness', 'sadness', 'surprise', 'neutral'], 'utterances': ['Hey! Hold on a minute, hold on a second. Do you think these pearls are nice?', \"I'd really prefer a mountain bike.\", \"Janice's birthday is coming up, I want to get her something speacial. Come in here with me.\", 'Whoa, whoa, whoa, wait, whoa.', 'Do you ah, want to get her something speacial, get her flowers, get her candy, get her gum, girls love gum.', \"That's a good idea, \\x91Dear Janice have a Hubba-Bubba birthday'. I would like to get her something serious.\", 'Oh, you want something serious.', \"Y'know what you should do, you should get her one of those um, barium enemas.\", 'Those are dead serious.', \"All right. Look, I'm gonna go in here, and you don't buy me anything ever.\", \"No, no, you can't, you can't, okay, you can't, you can't buy her pearls, you just can't, you can't, you can't.\", 'Why not?!', \"Oh God. Uh, okay, here's the thing, this is the thing, okay, the thing is...\", 'What is the thing?', \"Okay. I went down to the \\x91Mattress King' showroom and, and I saw Janice, kissing her ex-husband.\", 'What?', 'They were in his office.'], 'triggers': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"trainingEmotions = []\ntrainingSpeakers = []\ntrainingUtterances = []\nvalEmotions = []\nvalSpeakers = []\nvalUtterances = []\nfor example in trainingData:\n  for emotion in example['emotions']:\n    trainingEmotions.append(emotion)\n  for speaker in example['speakers']:\n    trainingSpeakers.append(speaker)\n  for utterance in example['utterances']:\n    trainingUtterances.append(utterance)\n\nfor example in valData:\n  for emotion in example['emotions']:\n    valEmotions.append(emotion)\n  for speaker in example['speakers']:\n    valSpeakers.append(speaker)\n  for utterance in example['utterances']:\n    valUtterances.append(utterance)\n\n# for example in valData:\n#   valEmotions.append(example['emotions'])\n#   valSpeakers.append(example['speakers'])\n#   valUtterances.append(example['utterances'])\n\nprint(len(valSpeakers))\nprint(len(trainingUtterances))\nprint(trainingEmotions[1])\nprint(len(trainingEmotions))\nprint(type(trainingEmotions[58956]))\nprint(valUtterances[1])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cigxkSjRIBxE","outputId":"5e2464d8-4720-4b0b-d141-bf1a4d8ef190","execution":{"iopub.status.busy":"2024-04-21T02:26:48.810814Z","iopub.execute_input":"2024-04-21T02:26:48.811124Z","iopub.status.idle":"2024-04-21T02:26:48.854914Z","shell.execute_reply.started":"2024-04-21T02:26:48.811097Z","shell.execute_reply":"2024-04-21T02:26:48.854053Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"7293\n58957\nfear\n58957\n<class 'str'>\nI'd really prefer a mountain bike.\n","output_type":"stream"}]},{"cell_type":"code","source":"DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"id":"--xNiSsEKwdd","execution":{"iopub.status.busy":"2024-04-21T02:26:48.856069Z","iopub.execute_input":"2024-04-21T02:26:48.856356Z","iopub.status.idle":"2024-04-21T02:26:48.892760Z","shell.execute_reply.started":"2024-04-21T02:26:48.856331Z","shell.execute_reply":"2024-04-21T02:26:48.891871Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2Model\n\ntrainInputs = [utterance for example in trainingData for utterance in example['utterances']]\ntrainLabels = [emotion for example in trainingData for emotion in example['emotions']]\nvalInputs = [utterance for example in valData for utterance in example['utterances']]\nvalLabels = [emotion for example in valData for emotion in example['emotions']]\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = GPT2Model.from_pretrained('gpt2')\n\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nclass CustomDataset(Dataset):\n    def __init__(self, inputs, labels, tokenizer, max_length=128):\n        self.inputs = inputs\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n    \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, idx):\n        input_text = self.inputs[idx]\n        label = self.labels[idx]\n        inputs = self.tokenizer.encode_plus(input_text, add_special_tokens=True, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n        return inputs, torch.tensor(label)\n\ntrainDataset = CustomDataset(trainInputs, trainLabels, tokenizer)\nvalDataset = CustomDataset(valInputs, valLabels, tokenizer)\n\ntrainLoader = DataLoader(trainDataset, batch_size=8, shuffle=True)\nvalLoader = DataLoader(valDataset, batch_size=8, shuffle=False)\n\nclass CustomClassifier(nn.Module):\n    def __init__(self, pretrained_model, num_labels):\n        super(CustomClassifier, self).__init__()\n        self.pretrained_model = pretrained_model\n        self.classifier = nn.Linear(self.pretrained_model.config.hidden_size, num_labels)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state.mean(dim=1)  # Using mean pooling over the last hidden state\n        logits = self.classifier(pooled_output)\n        return logits\n\nmodel = CustomClassifier(model, num_labels=7).to(DEVICE)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MbJ4DsXPEEKl","outputId":"bf491fd1-4bc8-4f95-8559-a81155941459","execution":{"iopub.status.busy":"2024-04-21T02:26:48.894002Z","iopub.execute_input":"2024-04-21T02:26:48.894709Z","iopub.status.idle":"2024-04-21T02:26:54.160949Z","shell.execute_reply.started":"2024-04-21T02:26:48.894667Z","shell.execute_reply":"2024-04-21T02:26:54.160153Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b8c9ee7ed324724b25900a9bc791cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bb706ce41d469c8999480f52430c93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1db6067df86a44bf85dc1916950f1ea1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"884bd723991a44188d448429764ed6bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"767ea1733f5a456ab7995de544773935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0d83f2a05b84efcbd29f629a626c230"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset\nfrom sklearn.preprocessing import LabelEncoder\n\ntrainInputs = tokenizer(trainingUtterances, padding = True, truncation = True, return_tensors = \"pt\")\ntempEncoder = LabelEncoder()\ntrainLabelsEncoded = tempEncoder.fit_transform(trainingEmotions)\nvalLabelsEncoded = tempEncoder.fit_transform(valEmotions)\ntrainLabels = torch.tensor(trainLabelsEncoded)\n\nvalInputs = tokenizer(valUtterances, padding = True, truncation = True, return_tensors = \"pt\")\nvalLabels = torch.tensor(valLabelsEncoded)\n\ntrainDataset = TensorDataset(trainInputs['input_ids'], trainInputs['attention_mask'], trainLabels)\nvalDataset = TensorDataset(valInputs['input_ids'], valInputs['attention_mask'], valLabels)\n\ntrainLoader = DataLoader(trainDataset, batch_size = 8, shuffle = True)\nvalLoader = DataLoader(valDataset, batch_size = 8, shuffle = True)","metadata":{"id":"ZjXH7WELEtH_","execution":{"iopub.status.busy":"2024-04-21T02:26:54.162055Z","iopub.execute_input":"2024-04-21T02:26:54.162341Z","iopub.status.idle":"2024-04-21T02:27:07.299832Z","shell.execute_reply.started":"2024-04-21T02:26:54.162316Z","shell.execute_reply":"2024-04-21T02:27:07.298941Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef train(model, epochCount, lossFn, opt, loader):\n    epochLoss = []\n    for epoch in range(epochCount):\n        model.train()\n        running_loss = 0.0\n        with tqdm(loader, desc=f\"Epoch {epoch+1}/{epochCount}\", unit=\"batch\") as tepoch:\n            for i, batch in enumerate(tepoch):\n                input_ids, attention_mask, labels = batch\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                labels = labels.to(DEVICE)\n\n                opt.zero_grad()\n\n                # Forward pass\n                logits = model(input_ids=input_ids, attention_mask=attention_mask)\n\n                # Calculate loss\n                loss = lossFn(logits, labels)\n                running_loss += loss.item()\n\n                # Backward pass\n                loss.backward()\n                opt.step()\n\n                # Print statistics every 1000 mini-batches\n                if i % 1000 == 999:  \n                    tepoch.set_postfix(loss=running_loss / 1000)\n                    running_loss = 0.0\n\n            # Calculate average loss for the epoch\n            epoch_loss = running_loss / len(loader)\n            epochLoss.append(epoch_loss)\n            tepoch.set_postfix(loss=epoch_loss)\n\n            # Save model checkpoint\n            torch.save({'epoch': epoch,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': opt.state_dict(),\n                        'loss': epoch_loss}, f'gpt2_classifier_epoch{epoch+1}.pth')\n\n    print('Training finished!')\n    return epochLoss\n\n# Define the number of epochs and the optimizer\nnumEpochs = 5\noptimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nlossFn = torch.nn.CrossEntropyLoss()\n\n# Train the model\ntrainingLossesList = train(model, numEpochs, lossFn, optimizer, trainLoader)","metadata":{"execution":{"iopub.status.busy":"2024-04-21T02:27:07.301211Z","iopub.execute_input":"2024-04-21T02:27:07.301611Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/5: 100%|██████████| 7370/7370 [12:17<00:00,  9.99batch/s, loss=0.298]\nEpoch 2/5: 100%|██████████| 7370/7370 [12:16<00:00, 10.00batch/s, loss=0.16] \nEpoch 3/5: 100%|██████████| 7370/7370 [12:16<00:00, 10.01batch/s, loss=0.127]\nEpoch 4/5: 100%|██████████| 7370/7370 [12:16<00:00, 10.00batch/s, loss=0.101]\nEpoch 5/5:  96%|█████████▌| 7071/7370 [11:47<00:29,  9.99batch/s, loss=0.0936]","output_type":"stream"}]},{"cell_type":"code","source":"print(trainingLossesList)","metadata":{"id":"XpzVln0cdi28","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot the training loss versus epoch\nplt.plot(range(1, numEpochs + 1), trainingLossesList, marker='o', linestyle='-')\nplt.title('Training Loss vs Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Training Loss')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finalTrainLoss = [1.6081185510637317, 1.6056980307907716, 1.6046199815972042, 1.6033065124719406, 1.6029525319303877]\nfinalTrainLoss = trainingLossesList\nfor i in range(len(finalTrainLoss)):\n  finalTrainLoss[i] = finalTrainLoss[i]*len(trainLoader)\n  finalTrainLoss[i] = finalTrainLoss[i]/(i+1)\n\nprint(finalTrainLoss)","metadata":{"id":"AGvJ0qdwdmjf","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reportTrainLoss = [11851.833721339703, 5916.997243463993, 3942.016421457132, 2954.0922492295504, 2362.7520320653916]","metadata":{"id":"rAJ8qs4p5CJ8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_model(model, optim, file_name):\n    checkpoint = torch.load(file_name, map_location=torch.device(DEVICE))\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optim.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n    return checkpoint[\"epoch\"]\n\ndef computeValLoss(model, loader, optimizer):\n    valLoss = []\n    for ep in range(5):\n        in_mod = '/kaggle/input/nlp-a4/bert_ERC_ep_val'+str(ep+1)+'.pth'  # Corrected the filename\n        checkpoint = torch.load(in_mod, map_location=torch.device(DEVICE))\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        model.eval()\n        curLoss = 0\n        with torch.no_grad():\n            for batch in loader:\n                input_ids, attention_mask, labels = batch\n                input_ids = input_ids.to(DEVICE)\n                attention_mask = attention_mask.to(DEVICE)\n                labels = labels.to(DEVICE)\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                curLoss += loss.item()\n\n        avgLoss = curLoss / len(loader)\n        valLoss.append(avgLoss)\n    return valLoss\n\n# Assuming you already have the model, optimizer, and loaders defined\n# Load the model and optimizer state from the last checkpoint\nepoch_resumed = load_model(model, optimizer, '/kaggle/input/nlp-a4/bert_ERC_ep5.pth')\n\n# Compute validation loss\nvalLosses = computeValLoss(model, valLoader, optimizer)\n\n# Plot the validation loss\nplt.plot(range(1, 6), valLosses, marker='o', linestyle='-')\nplt.title('Validation Loss vs Epoch')\nplt.xlabel('Epoch')\nplt.ylabel('Validation Loss')\nplt.grid(True)\nplt.show()\n","metadata":{"id":"NCZLVKMtAohB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nfinalValLoss = computeValLoss(model, valLoader, optimizer)\nprint(finalValLoss)","metadata":{"id":"DMSqiX8KA0RS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reportValLoss = [1463.82704859972, 730.9797016084194, 486.44409054517746, 365.1842586994171, 291.8854038953781]","metadata":{"id":"ZcRa3P5FDQKE","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_loc = '/kaggle/input/nlp-a4/bert_ERC_ep4.pth'\nload_model(model, torch.optim.Adam(model.parameters(), lr=0.001), model_loc)\n# predictions = []\n# model.eval()\n# with torch.inference_mode():\n#   for batch in valLoader:\n#     input_ids, attention_mask, labels = batch\n#     input_ids = input_ids.to(DEVICE)\n#     attention_mask = attention_mask.to(DEVICE)\n#     labels = labels.to(DEVICE)\n#     outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n#     for output in outputs:\n#       predictions.append(output)\n\n# print(predictions)\n\n############### INFERENCE CODE #######################\nwith open(\"/content/drive/MyDrive/NLP_A4/MELD_test_efr.json\", \"r\") as file:\n  testData = json.load(file)\n\ntestEmotions = []\ntestSpeakers = []\ntestUtterances = []\ntestConvoLength = []\nfor example in testData:\n  testConvoLength.append(len(example['emotions']))\n  for emotion in example['emotions']:\n    testEmotions.append(emotion)\n  for speaker in example['speakers']:\n    testSpeakers.append(speaker)\n  for utterance in example['utterances']:\n    testUtterances.append(utterance)\n\ntestInputs = tokenizer(testUtterances, padding = True, truncation = True, return_tensors = \"pt\")\ntestLabelsEncoded = tempEncoder.fit_transform(testEmotions)\ntestLabels = torch.tensor(testLabelsEncoded)\n\ntestDataset = TensorDataset(testInputs['input_ids'], testInputs['attention_mask'], testLabels)\ntestLoader = DataLoader(testDataset, batch_size = 8, shuffle = False)\n\n\n","metadata":{"id":"a8L0-VlwEObH","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, loader):\n    model.eval()\n    predictions = []\n    true_labels = []\n    for batch in loader:\n        input_ids, attention_mask, labels = batch\n        input_ids = input_ids.to(DEVICE)\n        attention_mask = attention_mask.to(DEVICE)\n        labels = labels.to(DEVICE)\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n            true_labels.extend(labels.cpu().numpy())\n    return predictions, true_labels","metadata":{"id":"sg0H4RZkszUI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom sklearn.metrics import f1_score\nfinalPredictions, finalLabels = evaluate(model.to(DEVICE), testLoader)\n\nmacro_f1 = f1_score(finalLabels, finalPredictions, average='macro')\nprint(f'Macro F1 Score: {macro_f1:.4f}')\n","metadata":{"id":"RGaBXlL7tSgT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"p5d1fqAr04d2"},"execution_count":null,"outputs":[]}]}